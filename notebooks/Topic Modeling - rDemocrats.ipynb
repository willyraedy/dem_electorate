{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.fetch_data import get_submission_docs_for_subreddit\n",
    "from data.clean_data import process_text\n",
    "\n",
    "%aimport data.fetch_data\n",
    "%aimport data.clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize, MWETokenizer # multi-word expression\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of comments by most prolific user: 0.02629016553067186\n"
     ]
    }
   ],
   "source": [
    "data_raw = get_submission_docs_for_subreddit('democrats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_raw.copy()\n",
    "data_clean.text = data_clean.text.map(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rDemocrats_data_clean.pickle', 'wb') as write_file:\n",
    "    pickle.dump(data_clean, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stop_words = [\n",
    "    'like', 'dont', 'im', 'say', 'did', 'said', 'thats', 'don', 'hes', 'does', 'thing', 'gt', 'sure', 'doesnt',\n",
    "    'saying', 'youre', 'isnt', 'doing', 'got', 'didnt', 'yeah', 'just', 'yes',\n",
    "    'right', 'think', 'going', 'want', 'know', 'good',\n",
    "    'need', 'time', 'point', 'make', 'way', 'really',\n",
    "    'id', 'ar', 's', 't', 've', 'm', 'shes', \n",
    "    'c', 'd', 'v', 'actually', 'look', 'maybe', 'though', 'bad', 'came', 'mods', 'things', 'lot', 'let', 'lol', 'tell', 'pretty', 'literally'\n",
    "    'theyre', 'people',\n",
    "    '‘', '’', '“'\n",
    "]\n",
    "multi_words = [\n",
    "    ('health','insurance'),\n",
    "    ('fox', 'news'),\n",
    "    ('bernie', 'sanders'),\n",
    "    ('hillary', 'clinton'),\n",
    "    ('barack', 'obama'),\n",
    "    ('donald', 'trump'),\n",
    "    ('joe', 'biden'),\n",
    "    ('joseph', 'biden'),\n",
    "    ('mass', 'shooting'),\n",
    "    ('mass', 'shootings'),\n",
    "    ('assault', 'weapon'),\n",
    "    ('assault', 'weapons'),\n",
    "    ('assault', 'weapons', 'ban'),\n",
    "    ('sergeant', 'at', 'arms'),\n",
    "    ('stop', 'and', 'frisk'),\n",
    "    ('medicare', 'for', 'all'),\n",
    "    ('public', 'option'),\n",
    "    ('beat', 'trump'),\n",
    "    ('articles', 'of', 'impeachment'),\n",
    "    ('new', 'york'),\n",
    "    ('hold', 'in', 'contempt'),\n",
    "    ('quid', 'pro', 'quo')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could do stemming, lemmatization, parts of speech, compound term extraction / named entity extraction, IF-IDF\n",
    "# lots of emoji's' now with nltk tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame(np.sum(data_dtm.transpose(), axis=1), columns=['word_count'], index=data_dtm.transpose().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_counts.word_count.sort_values(ascending=False).iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_counts.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lots of omitted spaces\n",
    "# fair number of spelling errors\n",
    "# problem is, TextBlob can't find omitted spaces and corrects things like \"pelosi\" to \"pelvis\"\n",
    "# omit for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\", topic_names[ix], \"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                         for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer(multi_words)\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(additional_stop_words)\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    tokenizer=lambda x: mwe_tokenizer.tokenize(word_tokenize(x)),\n",
    "    max_df=0.75\n",
    ")\n",
    "data_cv = cv.fit_transform(data_clean.text)\n",
    "data_dtm_raw = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm_raw.index = data_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm_raw.iloc[:5, -171:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove russian words, emojis, other weird stuff\n",
    "data_dtm = data_dtm_raw.iloc[:, :-170]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dim Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names_lsa = ['Election', 'Impeachment', 'Unclear - tax policy',\n",
    "                   'Unclear - maybe Health Care', 'Gun Violence/Assault Weapons ban']\n",
    "\n",
    "lsa = TruncatedSVD(5)\n",
    "doc_topic_lsa = lsa.fit_transform(data_dtm)\n",
    "print('Explained Variance Ratio: ', lsa.explained_variance_ratio_)\n",
    "display_topics(\n",
    "    lsa,\n",
    "    cv.get_feature_names()[:-171],\n",
    "    30,\n",
    "    topic_names=topic_names_lsa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vt = pd.DataFrame(doc_topic_lsa.round(5),\n",
    "                  #              index = example,\n",
    "                  columns=topic_names_lsa)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer(multi_words)\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(additional_stop_words)\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    tokenizer=lambda x: mwe_tokenizer.tokenize(word_tokenize(x)),\n",
    "    #     max_df=0.75\n",
    ")\n",
    "data_cv = cv.fit_transform(data_clean.text)\n",
    "data_dtm_raw = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm_raw.index = data_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zum</th>\n",
       "      <th>£</th>\n",
       "      <th>­</th>\n",
       "      <th>¯ツ¯</th>\n",
       "      <th>¿</th>\n",
       "      <th>¿philanthropist</th>\n",
       "      <th>élection</th>\n",
       "      <th>͜ʖ</th>\n",
       "      <th>͡ʘ</th>\n",
       "      <th>американец</th>\n",
       "      <th>говорить</th>\n",
       "      <th>как</th>\n",
       "      <th>–</th>\n",
       "      <th>—</th>\n",
       "      <th>—and</th>\n",
       "      <th>—donald</th>\n",
       "      <th>—gt</th>\n",
       "      <th>—gtevery</th>\n",
       "      <th>—the</th>\n",
       "      <th>—trump</th>\n",
       "      <th>—you</th>\n",
       "      <th>―</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zum  £  ­  ¯ツ¯  ¿  ¿philanthropist  élection  ͜ʖ  ͡ʘ  американец  говорить  \\\n",
       "0    0  0  0    0  0                0         0   0   0           0         0   \n",
       "1    0  0  0    0  0                1         0   0   0           0         0   \n",
       "3    0  0  0    0  0                0         0   0   0           0         0   \n",
       "4    0  0  0    0  0                0         0   0   0           0         0   \n",
       "6    0  0  0    0  0                0         0   0   0           0         0   \n",
       "\n",
       "   как  –  —  —and  —donald  —gt  —gtevery  —the  —trump  —you  ―  \n",
       "0    0  0  0     0        0    0         0     0       0     0  0  \n",
       "1    0  0  0     0        0    0         0     0       0     0  0  \n",
       "3    0  0  0     0        0    0         0     0       0     0  0  \n",
       "4    0  0  0     0        0    0         0     0       0     0  0  \n",
       "6    0  0  0     0        0    0         0     0       0     0  0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dtm_raw.iloc[:5, -172:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove russian words, emojis, other weird stuff\n",
    "data_dtm = data_dtm_raw.iloc[:, :-171]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dim Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic_labels = [\n",
    "    'frustration at 2016 election',\n",
    "    'impeachment hearings',\n",
    "    'election (candidates)',\n",
    "    'healthcare',\n",
    "    'gun control',\n",
    "    'election (high level general terms)',\n",
    "    'yang',\n",
    "    'impeachment',\n",
    "    'right wing media',\n",
    "    'debate',\n",
    "    'bipartisanship????',\n",
    "    'midwest elections?????',\n",
    "    'identity',\n",
    "    'Bloomberg',\n",
    "    'biden/ukraine',\n",
    "    'economy; trump vs. Obama credit',\n",
    "    'election (states)',\n",
    "    'monetary policy',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(18, random_state=42)\n",
    "doc_topic_nmf = nmf_model.fit_transform(data_dtm)\n",
    "\n",
    "display_topics(\n",
    "    nmf_model,\n",
    "    cv.get_feature_names()[:-171],\n",
    "    20,\n",
    "    topic_names=nmf_topic_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_topic = pd.DataFrame(doc_topic_nmf.round(5),\n",
    "                          #              index = example,\n",
    "                          columns=nmf_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_topic.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.lemmatize(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer(multi_words)\n",
    "\n",
    "def complete_tokenizer(x):\n",
    "    return mwe_tokenizer.tokenize(word_tokenize(x))\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(additional_stop_words)\n",
    "\n",
    "cv = StemmedCountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    tokenizer=complete_tokenizer,\n",
    "    #     max_df=0.75\n",
    ")\n",
    "data_cv = cv.fit_transform(data_clean.text)\n",
    "data_dtm_raw = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm_raw.index = data_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zum</th>\n",
       "      <th>£</th>\n",
       "      <th>­</th>\n",
       "      <th>¯ツ¯</th>\n",
       "      <th>¿</th>\n",
       "      <th>¿philanthropist</th>\n",
       "      <th>élection</th>\n",
       "      <th>͜ʖ</th>\n",
       "      <th>͡ʘ</th>\n",
       "      <th>американец</th>\n",
       "      <th>говорить</th>\n",
       "      <th>как</th>\n",
       "      <th>–</th>\n",
       "      <th>—</th>\n",
       "      <th>—and</th>\n",
       "      <th>—donald</th>\n",
       "      <th>—gt</th>\n",
       "      <th>—gtevery</th>\n",
       "      <th>—the</th>\n",
       "      <th>—trump</th>\n",
       "      <th>—you</th>\n",
       "      <th>―</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zum  £  ­  ¯ツ¯  ¿  ¿philanthropist  élection  ͜ʖ  ͡ʘ  американец  говорить  \\\n",
       "0    0  0  0    0  0                0         0   0   0           0         0   \n",
       "1    0  0  0    0  0                1         0   0   0           0         0   \n",
       "3    0  0  0    0  0                0         0   0   0           0         0   \n",
       "4    0  0  0    0  0                0         0   0   0           0         0   \n",
       "6    0  0  0    0  0                0         0   0   0           0         0   \n",
       "\n",
       "   как  –  —  —and  —donald  —gt  —gtevery  —the  —trump  —you  ―  \n",
       "0    0  0  0     0        0    0         0     0       0     0  0  \n",
       "1    0  0  0     0        0    0         0     0       0     0  0  \n",
       "3    0  0  0     0        0    0         0     0       0     0  0  \n",
       "4    0  0  0     0        0    0         0     0       0     0  0  \n",
       "6    0  0  0     0        0    0         0     0       0     0  0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dtm_raw.iloc[:5, -172:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove russian words, emojis, other weird stuff\n",
    "data_dtm = data_dtm_raw.iloc[:, :-171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic_labels = [\n",
    "    '2016_election_frustration',\n",
    "    'impeachment_proceedings',\n",
    "    'healthcare',\n",
    "    'primary_candidates',\n",
    "    'gun_control',\n",
    "    'election_general_terms',\n",
    "    'right_wing_media',\n",
    "    'impeachment',\n",
    "    'yang_ubi',\n",
    "    'primary_debates',\n",
    "    'bloomberg',\n",
    "    'econ_trump_vs_obama',\n",
    "    'race_identity',\n",
    "    'tax_return_ukraine_biden',\n",
    "    'election_midwest_swing',\n",
    "    'monetary_policy',\n",
    "    'rep_dem_comparison',\n",
    "    'miltary_and_immigration'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: ' 2016_election_frustration '\n",
      "trump, voted, win, fuck, supporter, election, hillary, vote, shit, lie, republican, war, believe, president, world, money, stupid, won, year, better\n",
      "\n",
      "Topic: ' impeachment_proceedings '\n",
      "power, congress, contempt, person, detain, court, house, arrest, supreme, senate, law, hold, sergeant_at_arms, authority, vote, jail, inherent, case, majority, dc\n",
      "\n",
      "Topic: ' healthcare '\n",
      "cost, tax, billion, healthcare, hospital, pay, rate, percent, spending, health, insurance, service, care, increase, medicare, million, medical, saving, private, paid\n",
      "\n",
      "Topic: ' primary_candidates '\n",
      "bernie, warren, sander, candidate, biden, supporter, vote, support, primary, progressive, poll, year, win, democratic, delegate, voter, hillary, campaign, literally, obama\n",
      "\n",
      "Topic: ' gun_control '\n",
      "gun, ban, law, weapon, rifle, firearm, democrat, death, used, mean, amendment, away, number, owner, right, control, common, use, stop, beto\n",
      "\n",
      "Topic: ' election_general_terms '\n",
      "vote, voting, voter, state, election, party, candidate, voted, win, black, democrat, government, right, democratic, country, ballot, responsibility, work, reason, live\n",
      "\n",
      "Topic: ' right_wing_media '\n",
      "hillary, bernie, year, hate, news, fact, fox, medium, shit, fox_news, clinton, cnn, act, lie, woman, support, cosponsored, link, work, democratic\n",
      "\n",
      "Topic: ' impeachment '\n",
      "senate, impeachment, trump, president, impeached, trial, republican, office, house, gop, crime, election, democrat, criminal, pelosi, vote, removed, power, mean, process\n",
      "\n",
      "Topic: ' yang_ubi '\n",
      "yang, ubi, policy, money, work, government, tax, pay, job, support, candidate, talk, better, year, vat, country, business, income, hour, care\n",
      "\n",
      "Topic: ' primary_debates '\n",
      "biden, candidate, debate, bernie, castro, question, trump, joe, harris, supporter, talk, answer, wrong, tonight, kid, pete, better, trying, yang, healthcare\n",
      "\n",
      "Topic: ' bloomberg '\n",
      "bloomberg, billionaire, money, new_york, experience, warren, city, mayor, candidate, democratic, campaign, fact, ad, running, policy, state, year, effective, stop_and_frisk, win\n",
      "\n",
      "Topic: ' econ_trump_vs_obama '\n",
      "president, job, trump, obama, economy, number, million, better, work, credit, lie, recovery, created, shit, fact, blame, promise, literally, total, trade\n",
      "\n",
      "Topic: ' race_identity '\n",
      "white, privilege, black, woman, man, day, male, history, mean, party, life, pete, men, straight, use, racist, post, problem, believe, gay\n",
      "\n",
      "Topic: ' tax_return_ukraine_biden '\n",
      "tax, return, public, private, money, republican, son, biden, year, citizen, government, business, president, wrong, politician, question, investigation, wealth, ukraine, care\n",
      "\n",
      "Topic: ' election_midwest_swing '\n",
      "state, poll, voter, election, win, mean, clinton, campaign, result, white, michigan, polling, swing, chance, far, winning, democrat, biden, primary, data\n",
      "\n",
      "Topic: ' monetary_policy '\n",
      "bank, inflation, money, asset, fed, rate, reserve, price, cash, wage, economy, minimum, higher, investment, pay, buy, mean, extra, rule, power\n",
      "\n",
      "Topic: ' rep_dem_comparison '\n",
      "republican, party, democrat, left, vote, gop, racist, thought, better, change, political, conservative, try, support, friend, belief, hate, democratic, ill, thank\n",
      "\n",
      "Topic: ' miltary_and_immigration '\n",
      "obama, military, federal, end, national, border, state, rule, rescinds, veteran, funding, year, program, war, nuclear, government, protection, reason, child, billion\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(18, random_state=42, alpha=0)\n",
    "doc_topic_nmf = nmf_model.fit_transform(data_dtm)\n",
    "\n",
    "display_topics(\n",
    "    nmf_model,\n",
    "    cv.get_feature_names(),\n",
    "    20,\n",
    "    topic_names=nmf_topic_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Winning Topic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rDemocrats_nmf.pickle', 'wb') as write_file:\n",
    "    pickle.dump(nmf_model, write_file)\n",
    "    \n",
    "with open('rDemocrats_CV.pickle', 'wb') as write_file:\n",
    "    pickle.dump(cv, write_file)\n",
    "    \n",
    "with open('rDemocrats_doc_topic.pickle', 'wb') as write_file:\n",
    "    pickle.dump(pd.DataFrame(doc_topic_nmf, columns=nmf_topic_labels), write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "class StemmedCountVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.lemmatize(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer(multi_words)\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(additional_stop_words)\n",
    "\n",
    "cv = StemmedCountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    tokenizer=lambda x: mwe_tokenizer.tokenize(word_tokenize(x)),\n",
    "    #     max_df=0.75\n",
    ")\n",
    "data_cv = cv.fit_transform(data_clean.text)\n",
    "data_dtm_raw = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm_raw.index = data_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(14, random_state=42, alpha=0)\n",
    "doc_topic_nmf = nmf_model.fit_transform(data_dtm)\n",
    "\n",
    "display_topics(\n",
    "    nmf_model,\n",
    "    cv.get_feature_names()[:-171],\n",
    "    20,\n",
    "#     topic_names=nmf_topic_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_clean = data.copy()\n",
    "data_clean.text = data.text.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.lemmatize(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer(multi_words)\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(additional_stop_words)\n",
    "\n",
    "cv = StemmedCountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    tokenizer=lambda x: mwe_tokenizer.tokenize(word_tokenize(x)),\n",
    "    #     max_df=0.75\n",
    ")\n",
    "data_cv = cv.fit_transform(data_clean.text)\n",
    "data_dtm_raw = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm_raw.index = data_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dtm_raw.iloc[:5, -172:-122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove russian words, emojis, other weird stuff\n",
    "data_dtm = data_dtm_raw.iloc[:, :-171]\n",
    "data_cv = data_cv[:, :-171]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Dim Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_word = data_cv.transpose()\n",
    "# haven't removed the russian and stuff\n",
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=18, id2word=id2word, passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
